{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4751b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "for g in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(g, True)\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb03b86",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes graph for reber string generation\n",
    "nodes = [\n",
    "    {\"current_node\" : 0, \"next_node\" : [(1, \"T\"), (2, \"P\")]},\n",
    "    {\"current_node\" : 1, \"next_node\" : [(1, \"S\"), (4, \"X\")]},\n",
    "    {\"current_node\" : 2, \"next_node\" : [(2, \"T\"), (3, \"V\")]},\n",
    "    {\"current_node\" : 3, \"next_node\" : [(4, \"P\"), (5, \"V\")]},\n",
    "    {\"current_node\" : 4, \"next_node\" : [(2, \"X\"), (5, \"S\")]},\n",
    "    {\"current_node\" : 5, \"next_node\" : [(6, \"E\"), (6, \"E\")]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ffb2627c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_node</th>\n",
       "      <th>next_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[(1, T), (2, P)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[(1, S), (4, X)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[(2, T), (3, V)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[(4, P), (5, V)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[(2, X), (5, S)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[(6, E), (6, E)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   current_node         next_node\n",
       "0             0  [(1, T), (2, P)]\n",
       "1             1  [(1, S), (4, X)]\n",
       "2             2  [(2, T), (3, V)]\n",
       "3             3  [(4, P), (5, V)]\n",
       "4             4  [(2, X), (5, S)]\n",
       "5             5  [(6, E), (6, E)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(nodes)\n",
    "df # to visualize the graph of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7e730456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def pick_random_length(max_char_count, mu=7.5, sigma=2.0):\n",
    "    lengths = np.arange(max_char_count)\n",
    "    # mu = 7.5 center around 7-8\n",
    "    # sigma = 2.0 adjust for spread; try 2.0 for a moderate peak\n",
    "\n",
    "    probabilities = norm.pdf(lengths, loc=mu, scale=sigma)\n",
    "    probabilities /= probabilities.sum()  # normalize\n",
    "\n",
    "    reber_lenght = np.random.choice(lengths, p=probabilities)\n",
    "    return reber_lenght\n",
    "\n",
    "def pick_path(randomized_node=False):\n",
    "    path_or_node = np.random.randint(0, 2) if randomized_node == False else np.random.randint(0, 6)\n",
    "    return path_or_node\n",
    "\n",
    "\n",
    "def generate_reber_string(nodes, is_reber=True, is_generator=False, **kwargs):\n",
    "    \"\"\"This method creates one instance of reber string, returns a string a tuple (reber, is_reber) or\n",
    "    yields a tuple (reber, is_reber). Returns string only and only if is_reber isn't random\n",
    "    and is_generator=False\"\"\"\n",
    "\n",
    "    def create_reber_string(nodes, is_reber):\n",
    "        node = 0\n",
    "        reber = \"B\"\n",
    "        max_char_count = kwargs.get(\"max_char_count\", 16)\n",
    "        \n",
    "        # if is_reber true\n",
    "        if is_reber:\n",
    "            while node < 6:\n",
    "                selected_path = pick_path()\n",
    "                label = nodes[node][\"next_node\"][selected_path][1]\n",
    "                node = nodes[node][\"next_node\"][selected_path][0] \n",
    "                if isinstance(label, list):\n",
    "                    # this string is for inside the embedded reber\n",
    "                    inner_reber = generate_reber_string(label)\n",
    "                    reber += inner_reber\n",
    "                else:\n",
    "                    reber += label\n",
    "        # else scope is quite overkill, I wont use\n",
    "        else:\n",
    "            try:            \n",
    "                mistake_count = 0\n",
    "                char_count = 0\n",
    "                while (node < 6 or mistake_count == 0) and char_count <= max_char_count:\n",
    "                    selected_path = pick_path()\n",
    "                    label = nodes[node][\"next_node\"][selected_path][1] # either list or string\n",
    "                    if isinstance(label, list):\n",
    "                        # this string is for inside the embedded reber\n",
    "                        inner_reber = generate_reber_string(label)\n",
    "                        reber += inner_reber\n",
    "                    else: # if string just add to reber\n",
    "                        reber += label\n",
    "                        if node != 6:\n",
    "                            random_node = pick_path(True)\n",
    "                            mistake_count += 1 if random_node != node else 0\n",
    "                        else:\n",
    "                            node = pick_path(True)\n",
    "                        char_count += 1\n",
    "                reber = reber[:pick_random_length(max_char_count)]\n",
    "            except IndexError:\n",
    "                print(f\"IndexError : {node}, {reber}, {mistake_count}\")\n",
    "        \n",
    "        return reber\n",
    "    \n",
    "    if not is_generator:\n",
    "        if is_reber == \"random\":\n",
    "            is_reber = bool(pick_path()) \n",
    "            return create_reber_string(nodes, is_reber), is_reber\n",
    "        else:\n",
    "            return create_reber_string(nodes, is_reber)\n",
    "    else:\n",
    "        dataset_size = kwargs.get(\"dataset_size\", 10000)\n",
    "        if is_reber == \"random\":\n",
    "            is_reber = (bool(pick_path()) for i in range(dataset_size))\n",
    "        return ((create_reber_string(nodes, is_reber), is_reber) for _ in range(dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "021e78bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVPXTVPXTTVPSE\n",
      "BPTVPSE\n",
      "BPVVE\n",
      "BPVPXVVE\n",
      "BTXXTTTTVVE\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(generate_reber_string(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5583c374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPTVVE True\n",
      "BTXXVVE True\n",
      "BTTTPPPT False\n",
      "BTSXSE True\n",
      "BPVVE True\n",
      "BPTTPPPPT False\n",
      "BTTTTPPP False\n",
      "BPPTPPP False\n",
      "BTXSE True\n",
      "BTSXXTTTVPSE True\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    is_reber = bool(pick_path())\n",
    "    print(generate_reber_string(nodes, is_reber), is_reber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_nodes = [\n",
    "    {\"current_node\" : 0, \"next_node\" : [(1, \"T\"), (2, \"P\")]},\n",
    "    {\"current_node\" : 1, \"next_node\" : [(4, nodes), \n",
    "                                        (4, nodes)]},\n",
    "    {\"current_node\" : 2, \"next_node\" : [(3, nodes), \n",
    "                                        (3, nodes)]},\n",
    "    {\"current_node\" : 3, \"next_node\" : [(5, \"P\"), (5, \"P\")]},\n",
    "    {\"current_node\" : 4, \"next_node\" : [(5, \"T\"), (5, \"T\")]},\n",
    "    {\"current_node\" : 5, \"next_node\" : [(6, \"E\"), (6, \"E\")]}\n",
    "]\n",
    "\n",
    "def create_embedded_reber(embedded_nodes, can_corrupt=True, **kwargs):\n",
    "    dataset_size = kwargs.get(\"dataset_size\", 10000)\n",
    "    POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "\n",
    "    for i in range(dataset_size):\n",
    "        embedded_reber_str = \"\"\n",
    "        if can_corrupt is True:\n",
    "            corrupt = bool(pick_path()) # Decide this single instance will be corrupted or not\n",
    "            embedded_reber_str += generate_reber_string(embedded_nodes)\n",
    "            # if this is instance is chosen to be corrupted\n",
    "            if corrupt is True:\n",
    "                corrupter_char = np.random.choice(list(POSSIBLE_CHARS))\n",
    "                idx = np.random.randint(0, len(embedded_reber_str))\n",
    "                while embedded_reber_str[idx] == corrupter_char:\n",
    "                    idx = np.random.randint(0, len(embedded_reber_str))\n",
    "                embedded_reber_str = embedded_reber_str[:idx] + corrupter_char + embedded_reber_str[idx+1:]\n",
    "            yield embedded_reber_str, corrupt # which is label \n",
    "        else:\n",
    "            embedded_reber_str += generate_reber_string(embedded_nodes)\n",
    "            yield embedded_reber_str, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cb6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: create_embedded_reber(embedded_reber_nodes, can_corrupt=True, dataset_size=10000),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.bool)\n",
    "    )\n",
    ").shuffle(10000).repeat() # infinite number of instances dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'BTBPVPXTTTVVTTE', shape=(), dtype=string) tf.Tensor(True, shape=(), dtype=bool)\n",
      "tf.Tensor(b'BPBPVVEPE', shape=(), dtype=string) tf.Tensor(False, shape=(), dtype=bool)\n",
      "tf.Tensor(b'BPETXSEPE', shape=(), dtype=string) tf.Tensor(True, shape=(), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(3):\n",
    "    print(x, y) # it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2229d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "POSSIBLE_CHARS = \"BEPSTVX\" \n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    standardize=None,\n",
    "    split=\"character\",\n",
    "    vocabulary=list(POSSIBLE_CHARS),  \n",
    "    output_mode=\"int\",\n",
    ")\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = vectorizer(x)\n",
    "    x = tf.cast(x, tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "757fa425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ParallelMapDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.bool, name=None))>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc94454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(size=10000, batch_size=32, can_corrupt=True):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: create_embedded_reber(embedded_reber_nodes, can_corrupt=can_corrupt, dataset_size=size),\n",
    "        output_signature=(tf.TensorSpec((), tf.string), tf.TensorSpec((), tf.bool))\n",
    "    )\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=AUTOTUNE).padded_batch(\n",
    "            batch_size,\n",
    "            padded_shapes=([None], []),\n",
    "            padding_values=(tf.constant(0, tf.int32), tf.constant(False))\n",
    "    ).repeat().prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b68c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 10000\n",
    "val_size = 2000\n",
    "batch_size = 32\n",
    "\n",
    "train_set = create_dataset(train_size, batch_size)\n",
    "valid_set = create_dataset(val_size, batch_size)\n",
    "test_set  = create_dataset(1000, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90f14a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ddd7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_size // batch_size + 1\n",
    "validation_steps = val_size // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68a5bdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = vectorizer.vocabulary_size()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1fa27fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model\n",
    "\n",
    "inputs = layers.Input(shape=(None,), dtype=\"int32\")  \n",
    "x = layers.Embedding(vocab_size, 5, mask_zero=True)(inputs)\n",
    "x = layers.GRU(32, return_sequences=True)(x)\n",
    "x = layers.GRU(16)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898716d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - accuracy: 0.5470 - loss: 0.6762 - val_accuracy: 0.6130 - val_loss: 0.6455\n",
      "Epoch 2/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.6298 - loss: 0.6214 - val_accuracy: 0.6590 - val_loss: 0.5916\n",
      "Epoch 3/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.7452 - loss: 0.5102 - val_accuracy: 0.8310 - val_loss: 0.4025\n",
      "Epoch 4/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.8512 - loss: 0.3520 - val_accuracy: 0.8595 - val_loss: 0.3269\n",
      "Epoch 5/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8985 - loss: 0.2650 - val_accuracy: 0.8985 - val_loss: 0.2348\n",
      "Epoch 6/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9234 - loss: 0.2117 - val_accuracy: 0.9365 - val_loss: 0.1792\n",
      "Epoch 7/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9373 - loss: 0.1831 - val_accuracy: 0.9425 - val_loss: 0.1615\n",
      "Epoch 8/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9487 - loss: 0.1585 - val_accuracy: 0.9485 - val_loss: 0.1540\n",
      "Epoch 9/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9521 - loss: 0.1456 - val_accuracy: 0.9660 - val_loss: 0.1300\n",
      "Epoch 10/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9612 - loss: 0.1179 - val_accuracy: 0.9680 - val_loss: 0.1123\n",
      "Epoch 11/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 24ms/step - accuracy: 0.9691 - loss: 0.0992 - val_accuracy: 0.9750 - val_loss: 0.0770\n",
      "Epoch 12/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9774 - loss: 0.0737 - val_accuracy: 0.9610 - val_loss: 0.0943\n",
      "Epoch 13/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9857 - loss: 0.0455 - val_accuracy: 0.9950 - val_loss: 0.0274\n",
      "Epoch 14/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9941 - loss: 0.0220 - val_accuracy: 0.9905 - val_loss: 0.0340\n",
      "Epoch 15/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9972 - loss: 0.0110 - val_accuracy: 0.9980 - val_loss: 0.0089\n",
      "Epoch 16/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9953 - loss: 0.0159 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
      "Epoch 17/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step - accuracy: 0.9977 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 18/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9995 - loss: 0.0025 - val_accuracy: 0.9985 - val_loss: 0.0048\n",
      "Epoch 19/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9966 - loss: 0.0117 - val_accuracy: 0.9985 - val_loss: 0.0059\n",
      "Epoch 20/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 25ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 8.7497e-04\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_set,\n",
    "    validation_steps=validation_steps\n",
    ") # accuracy: 0.9997 val_accuracy: 1.0000 in epoch 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl2wsl (Py3.11)",
   "language": "python",
   "name": "homl2wsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
